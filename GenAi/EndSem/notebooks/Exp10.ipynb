{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a288ee43",
   "metadata": {},
   "source": [
    "### Applying Model Optimization Techniques (Pruning & Quantization) to a Pre-trained Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809e62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import time\n",
    "import psutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a309f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "def prune_model(model):\n",
    "    # Prune 20% of weights in linear layers\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "    return model\n",
    "\n",
    "def quantize_model(model):\n",
    "    # Dynamic quantization for better compatibility\n",
    "    model.eval()\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df60798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, prompt=\"Hello world\", num_runs=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    start_time = time.time()\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    memory_after = get_memory_usage()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    memory_used = memory_after - memory_before\n",
    "    return avg_time, memory_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0cc209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "  Inference time: 0.0945 s\n",
      "  Memory usage: 311.01 MB\n",
      "\n",
      "After pruning:\n",
      "  Inference time: 0.0764 s\n",
      "  Memory usage: 220.63 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24884\\101420457.py:15: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_model = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After quantization:\n",
      "  Inference time: 0.0356 s\n",
      "  Memory usage: 3.64 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(\"Original model:\")\n",
    "orig_time, orig_mem = benchmark_inference(original_model, tokenizer)\n",
    "print(f\"  Inference time: {orig_time:.4f} s\")\n",
    "print(f\"  Memory usage: {orig_mem:.2f} MB\")\n",
    "    \n",
    "# Apply pruning to a copy\n",
    "pruned_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "pruned_model = prune_model(pruned_model)\n",
    "print(\"\\nAfter pruning:\")\n",
    "prune_time, prune_mem = benchmark_inference(pruned_model, tokenizer)\n",
    "print(f\"  Inference time: {prune_time:.4f} s\")\n",
    "print(f\"  Memory usage: {prune_mem:.2f} MB\")\n",
    "    \n",
    "# Apply quantization to original\n",
    "quantized_model = quantize_model(original_model)\n",
    "print(\"\\nAfter quantization:\")\n",
    "quant_time, quant_mem = benchmark_inference(quantized_model, tokenizer)\n",
    "print(f\"  Inference time: {quant_time:.4f} s\")\n",
    "print(f\"  Memory usage: {quant_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245ec68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
