{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6010ba4d",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Transformer-based Text Generation Model using LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b578afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Academics\\Aissms\\B.Tech.Docs\\Sem-7\\Practical\\GenAi\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeae71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(model, tokenizer, text, device):\n",
    "    \"\"\"Evaluate model perplexity on given text\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = math.exp(loss.item())\n",
    "    return loss.item(), perplexity\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, device, max_length=100):\n",
    "    \"\"\"Generate text using the model\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e854afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small training corpus for fine-tuning\n",
    "training_corpus = [\n",
    "    \"In a distant galaxy far away, space explorers discovered new worlds.\",\n",
    "    \"The brave astronauts ventured into uncharted territories of the cosmos.\",\n",
    "    \"Among the stars, ancient civilizations left mysterious artifacts.\",\n",
    "    \"Interstellar travel opened new possibilities for human expansion.\",\n",
    "    \"The spaceship navigated through asteroid fields with precision.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41a3001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING BASE MODEL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASE MODEL EVALUATION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: In a distant galaxy, it’s time to set about how it is made possible’s creation. And while the galaxy’s first galaxies are still around in the solar system, their vast array of galaxies, as well as their vast array of galaxies, has allowed astronomers to quickly create new ways to look at these galaxies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“The Universe is a huge planet and it has massive clouds and clouds of stars,’’’’\n",
      "Base Model - Loss: 4.0832, Perplexity: 59.3349\n",
      "\n",
      "=== APPLYING LoRA CONFIGURATION ===\n",
      "trainable params: 405,504 || all params: 82,318,080 || trainable%: 0.4926\n",
      "\n",
      "=== FINE-TUNING WITH LoRA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academics\\Aissms\\B.Tech.Docs\\Sem-7\\Practical\\GenAi\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 5.5796\n",
      "Epoch 2: Average Loss = 5.5584\n",
      "Epoch 2: Average Loss = 5.5584\n",
      "Epoch 3: Average Loss = 5.5602\n",
      "\n",
      "=== FINE-TUNED MODEL EVALUATION ===\n",
      "Epoch 3: Average Loss = 5.5602\n",
      "\n",
      "=== FINE-TUNED MODEL EVALUATION ===\n",
      "Generated Text: In a distant galaxy, an asteroid, a spacecraft, or an asteroid with a solar radius of about one million kilometers, is a common sight in science fiction. It's a sight, as seen by astronomers in the far reaches of space.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuned Model - Loss: 4.0275, Perplexity: 56.1216\n",
      "\n",
      "=== COMPARISON SUMMARY ===\n",
      "Base Model      - Perplexity: 59.3349\n",
      "Fine-tuned Model - Perplexity: 56.1216\n",
      "Improvement: 5.42%\n",
      "Generated Text: In a distant galaxy, an asteroid, a spacecraft, or an asteroid with a solar radius of about one million kilometers, is a common sight in science fiction. It's a sight, as seen by astronomers in the far reaches of space.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuned Model - Loss: 4.0275, Perplexity: 56.1216\n",
      "\n",
      "=== COMPARISON SUMMARY ===\n",
      "Base Model      - Perplexity: 59.3349\n",
      "Fine-tuned Model - Perplexity: 56.1216\n",
      "Improvement: 5.42%\n"
     ]
    }
   ],
   "source": [
    "# LoRA Fine-tuning Implementation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "print(\"=== LOADING BASE MODEL ===\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "\n",
    "# Evaluate base model\n",
    "test_prompt = \"In a distant galaxy,\"\n",
    "print(f\"\\n=== BASE MODEL EVALUATION ===\")\n",
    "base_text = generate_text(model, tokenizer, test_prompt, device)\n",
    "print(\"Generated Text:\", base_text)\n",
    "\n",
    "# Calculate perplexity on training corpus\n",
    "corpus_text = \" \".join(training_corpus)\n",
    "base_loss, base_perplexity = evaluate_perplexity(model, tokenizer, corpus_text, device)\n",
    "print(f\"Base Model - Loss: {base_loss:.4f}, Perplexity: {base_perplexity:.4f}\")\n",
    "\n",
    "# Setup LoRA configuration\n",
    "print(f\"\\n=== APPLYING LoRA CONFIGURATION ===\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Simple fine-tuning on corpus (minimal training loop)\n",
    "print(f\"\\n=== FINE-TUNING WITH LoRA ===\")\n",
    "lora_model.train()\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):  # Few epochs for demonstration\n",
    "    total_loss = 0\n",
    "    for text in training_corpus:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "        outputs = lora_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(training_corpus)\n",
    "    print(f\"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "print(f\"\\n=== FINE-TUNED MODEL EVALUATION ===\")\n",
    "lora_model.eval()\n",
    "finetuned_text = generate_text(lora_model, tokenizer, test_prompt, device)\n",
    "print(\"Generated Text:\", finetuned_text)\n",
    "\n",
    "finetuned_loss, finetuned_perplexity = evaluate_perplexity(lora_model, tokenizer, corpus_text, device)\n",
    "print(f\"Fine-tuned Model - Loss: {finetuned_loss:.4f}, Perplexity: {finetuned_perplexity:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\n=== COMPARISON SUMMARY ===\")\n",
    "print(f\"Base Model      - Perplexity: {base_perplexity:.4f}\")\n",
    "print(f\"Fine-tuned Model - Perplexity: {finetuned_perplexity:.4f}\")\n",
    "print(f\"Improvement: {((base_perplexity - finetuned_perplexity) / base_perplexity * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f755a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
